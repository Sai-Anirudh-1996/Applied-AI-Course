{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_2KJP0cU3Pf"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Apz6L51eU3Pp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrPHM8KoU3Py"
   },
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTsh4UddU3P6"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EX5GVWTwU3P9"
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDkeqaI8U3QF"
   },
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQCIfb5uU3QP"
   },
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'/content/drive/My Drive/HumanActivityRecognition.zip (Unzipped Files)/HAR/UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).as_matrix()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2s_Tei7FU3Qc"
   },
   "outputs": [],
   "source": [
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'/content/drive/My Drive/HumanActivityRecognition.zip (Unzipped Files)/HAR/UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJHphoNsU3Ql"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "gkyTVSMtU3Qs",
    "outputId": "cb42e13f-a14c-4b6f-e8e3-bb1b10d119de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88oa5oikU3Q3"
   },
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H1gTCC5kU3RA",
    "outputId": "e1bbb424-0afc-48cd-9473-0bc20e6af8cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSJtefZ0U3RH"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__p9wdWRU3RP"
   },
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "n_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaEhs1DjU3RW"
   },
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "cviOar2E72wG",
    "outputId": "067194cc-f569-4174-89fc-294412dc1ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Lh2D0x24U3Re",
    "outputId": "6782c3cb-958f-45aa-8d8d-5fbfcac8d441"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "XtTj2szvU3Rn",
    "outputId": "97747099-89c1-4685-9907-ee5f7ee2b59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q99PA2TpU3R4"
   },
   "source": [
    "- Defining the Architecture of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "okJCX2PQU3R6",
    "outputId": "820d906d-e895-4f10-b0c3-fe74cc63fe9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 32)                5376      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 5,574\n",
      "Trainable params: 5,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "CpntV2JeU3SC",
    "outputId": "965aa5f7-c6dc-4708-a27c-3bcd3c66de4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8pMOscF6U3SJ",
    "outputId": "25488b44-4cf5-474b-9535-b68e1f0333a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 1.3516 - acc: 0.4373 - val_loss: 1.1546 - val_acc: 0.4968\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.9905 - acc: 0.5813 - val_loss: 0.9254 - val_acc: 0.5789\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.7942 - acc: 0.6484 - val_loss: 0.8159 - val_acc: 0.6138\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.7300 - acc: 0.6591 - val_loss: 0.7393 - val_acc: 0.6162\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.6787 - acc: 0.6634 - val_loss: 0.7693 - val_acc: 0.6223\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.6435 - acc: 0.6684 - val_loss: 0.7687 - val_acc: 0.6026\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.6228 - acc: 0.6753 - val_loss: 0.7783 - val_acc: 0.6108\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.5839 - acc: 0.6857 - val_loss: 0.6796 - val_acc: 0.6254\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.6028 - acc: 0.6772 - val_loss: 0.6275 - val_acc: 0.6223\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.5479 - acc: 0.6995 - val_loss: 0.5836 - val_acc: 0.6271\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.5847 - acc: 0.6880 - val_loss: 0.6025 - val_acc: 0.6216\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.5360 - acc: 0.7121 - val_loss: 0.5962 - val_acc: 0.7309\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.5376 - acc: 0.7466 - val_loss: 0.6750 - val_acc: 0.6695\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.4910 - acc: 0.7730 - val_loss: 0.6172 - val_acc: 0.7360\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.6005 - acc: 0.7155 - val_loss: 0.6684 - val_acc: 0.7092\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.5334 - acc: 0.7384 - val_loss: 0.6147 - val_acc: 0.7275\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.4631 - acc: 0.7801 - val_loss: 0.5454 - val_acc: 0.7170\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.4275 - acc: 0.8165 - val_loss: 0.4400 - val_acc: 0.8259\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.3822 - acc: 0.8604 - val_loss: 0.6053 - val_acc: 0.8035\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.3829 - acc: 0.8704 - val_loss: 0.5289 - val_acc: 0.8334\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.3143 - acc: 0.9040 - val_loss: 0.4310 - val_acc: 0.8683\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.2789 - acc: 0.9149 - val_loss: 0.4263 - val_acc: 0.8758\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.2516 - acc: 0.9236 - val_loss: 0.3736 - val_acc: 0.8918\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.2199 - acc: 0.9336 - val_loss: 0.4395 - val_acc: 0.8799\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.4653 - acc: 0.8814 - val_loss: 0.7799 - val_acc: 0.7048\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.3091 - acc: 0.8962 - val_loss: 0.4278 - val_acc: 0.8948\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.2075 - acc: 0.9327 - val_loss: 0.4168 - val_acc: 0.8931\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.2721 - acc: 0.9323 - val_loss: 0.3600 - val_acc: 0.8921\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1831 - acc: 0.9423 - val_loss: 0.3449 - val_acc: 0.9050\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1574 - acc: 0.9446 - val_loss: 0.3871 - val_acc: 0.8941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f946b2b3dd8>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "nCVpnZT0U3SR",
    "outputId": "9d7115ec-7b44-4596-a512-c2c693640ac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 510        0  ...                   0                 0\n",
      "SITTING                  0      407  ...                   0                 0\n",
      "STANDING                 0      106  ...                   0                 0\n",
      "WALKING                  0        1  ...                  10                14\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 384                11\n",
      "WALKING_UPSTAIRS         0        6  ...                  13               437\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5dHLJ_aWU3SZ",
    "outputId": "33f1f4eb-e4b2-45a3-c92c-c2cdf632dca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 6s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1z1vG2tJU3Se",
    "outputId": "bf8ac6f9-3c24-46ef-993e-a326ae80f9d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38706818539458054, 0.8941296233457754]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrltl8boU3Sn"
   },
   "source": [
    "- With a simple 2 layer architecture we got 90.09% accuracy and a loss of 0.30\n",
    "- We can further imporve the performace with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MTtV9dJJHtJg",
    "outputId": "018ed251-9189-4e5d-c625-7d6b2d951044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 1.6115 - acc: 0.3658 - val_loss: 1.4700 - val_acc: 0.4751\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.4646 - acc: 0.3817 - val_loss: 1.3813 - val_acc: 0.3811\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.3209 - acc: 0.4368 - val_loss: 1.2607 - val_acc: 0.4333\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 1.2147 - acc: 0.4718 - val_loss: 1.1651 - val_acc: 0.4873\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.1292 - acc: 0.4815 - val_loss: 1.0656 - val_acc: 0.4893\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.0493 - acc: 0.5054 - val_loss: 0.9974 - val_acc: 0.4815\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.0302 - acc: 0.5137 - val_loss: 0.9866 - val_acc: 0.4852\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.0392 - acc: 0.5132 - val_loss: 1.1052 - val_acc: 0.4686\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.1361 - acc: 0.4693 - val_loss: 1.2062 - val_acc: 0.4181\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.1476 - acc: 0.4646 - val_loss: 1.0932 - val_acc: 0.4537\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.0795 - acc: 0.4819 - val_loss: 1.0856 - val_acc: 0.4547\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.9943 - acc: 0.5135 - val_loss: 0.9573 - val_acc: 0.4788\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.8996 - acc: 0.5522 - val_loss: 0.8659 - val_acc: 0.5110\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.9371 - acc: 0.5390 - val_loss: 0.8346 - val_acc: 0.5446\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.8492 - acc: 0.5626 - val_loss: 0.8599 - val_acc: 0.5857\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.8768 - acc: 0.5720 - val_loss: 0.8053 - val_acc: 0.5965\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.8181 - acc: 0.5933 - val_loss: 0.7871 - val_acc: 0.5986\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.8188 - acc: 0.5993 - val_loss: 0.7583 - val_acc: 0.6084\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.8372 - acc: 0.6084 - val_loss: 0.7541 - val_acc: 0.6040\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.7862 - acc: 0.6213 - val_loss: 0.7403 - val_acc: 0.6091\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.7781 - acc: 0.6228 - val_loss: 0.7551 - val_acc: 0.5969\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.7678 - acc: 0.6198 - val_loss: 0.7401 - val_acc: 0.6071\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.7671 - acc: 0.6198 - val_loss: 0.7305 - val_acc: 0.6125\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.7997 - acc: 0.6209 - val_loss: 0.8436 - val_acc: 0.5911\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.8530 - acc: 0.6055 - val_loss: 0.7331 - val_acc: 0.6040\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.7705 - acc: 0.6194 - val_loss: 0.7384 - val_acc: 0.6020\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.8240 - acc: 0.6205 - val_loss: 0.7109 - val_acc: 0.5999\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.7463 - acc: 0.6227 - val_loss: 0.7307 - val_acc: 0.5914\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.7424 - acc: 0.6340 - val_loss: 0.7276 - val_acc: 0.6284\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.7566 - acc: 0.6245 - val_loss: 0.7300 - val_acc: 0.6060\n",
      "16\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 1.4952 - acc: 0.3152 - val_loss: 1.4323 - val_acc: 0.3390\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.3326 - acc: 0.3560 - val_loss: 1.3781 - val_acc: 0.3702\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.2501 - acc: 0.4286 - val_loss: 1.2120 - val_acc: 0.4432\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.1672 - acc: 0.4737 - val_loss: 1.3350 - val_acc: 0.3936\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.1212 - acc: 0.4830 - val_loss: 1.1832 - val_acc: 0.4367\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.0875 - acc: 0.4883 - val_loss: 1.1543 - val_acc: 0.4574\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.0785 - acc: 0.4993 - val_loss: 1.1325 - val_acc: 0.4642\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.1172 - acc: 0.4894 - val_loss: 1.3468 - val_acc: 0.4537\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.0977 - acc: 0.4921 - val_loss: 1.1571 - val_acc: 0.4734\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.0596 - acc: 0.5005 - val_loss: 1.1165 - val_acc: 0.4815\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.0656 - acc: 0.5014 - val_loss: 1.1354 - val_acc: 0.4846\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.0744 - acc: 0.4997 - val_loss: 1.1299 - val_acc: 0.4886\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.9742 - acc: 0.5507 - val_loss: 1.0237 - val_acc: 0.5782\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.9422 - acc: 0.5637 - val_loss: 0.9627 - val_acc: 0.5490\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.8422 - acc: 0.6201 - val_loss: 0.8366 - val_acc: 0.6152\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.7864 - acc: 0.6340 - val_loss: 0.7761 - val_acc: 0.6138\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.7401 - acc: 0.6419 - val_loss: 0.7767 - val_acc: 0.6142\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.7117 - acc: 0.6500 - val_loss: 0.6973 - val_acc: 0.6206\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.7160 - acc: 0.6487 - val_loss: 0.6772 - val_acc: 0.6345\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.7045 - acc: 0.6555 - val_loss: 0.6658 - val_acc: 0.6200\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.6701 - acc: 0.6730 - val_loss: 0.6599 - val_acc: 0.6274\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.6413 - acc: 0.6738 - val_loss: 0.6502 - val_acc: 0.6342\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.6324 - acc: 0.6780 - val_loss: 0.6552 - val_acc: 0.6223\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.6276 - acc: 0.6880 - val_loss: 0.6841 - val_acc: 0.6271\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.6021 - acc: 0.7035 - val_loss: 0.6085 - val_acc: 0.6407\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.5934 - acc: 0.7201 - val_loss: 0.6170 - val_acc: 0.6698\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.5709 - acc: 0.7330 - val_loss: 0.7641 - val_acc: 0.7177\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.5621 - acc: 0.7486 - val_loss: 0.6995 - val_acc: 0.7648\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.5546 - acc: 0.7503 - val_loss: 0.6287 - val_acc: 0.7747\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.5403 - acc: 0.7711 - val_loss: 0.6713 - val_acc: 0.7794\n",
      "64\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 90s 12ms/step - loss: 1.2594 - acc: 0.4445 - val_loss: 1.1076 - val_acc: 0.5402\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.8747 - acc: 0.6026 - val_loss: 0.8584 - val_acc: 0.6200\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.7547 - acc: 0.6504 - val_loss: 0.7633 - val_acc: 0.6990\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.6354 - acc: 0.7186 - val_loss: 0.6649 - val_acc: 0.7394\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.5472 - acc: 0.7607 - val_loss: 0.6185 - val_acc: 0.7472\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.5001 - acc: 0.7889 - val_loss: 0.6418 - val_acc: 0.7791\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.3639 - acc: 0.8779 - val_loss: 0.5364 - val_acc: 0.8103\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2587 - acc: 0.9191 - val_loss: 0.7236 - val_acc: 0.7930\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2288 - acc: 0.9291 - val_loss: 0.3999 - val_acc: 0.8884\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2069 - acc: 0.9300 - val_loss: 0.5649 - val_acc: 0.8449\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2406 - acc: 0.9227 - val_loss: 0.4199 - val_acc: 0.8758\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1998 - acc: 0.9366 - val_loss: 0.3100 - val_acc: 0.8972\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1915 - acc: 0.9392 - val_loss: 0.2973 - val_acc: 0.9023\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1782 - acc: 0.9411 - val_loss: 0.2894 - val_acc: 0.9094\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1578 - acc: 0.9446 - val_loss: 0.3312 - val_acc: 0.9023\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1641 - acc: 0.9427 - val_loss: 0.3095 - val_acc: 0.9002\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1486 - acc: 0.9467 - val_loss: 0.3037 - val_acc: 0.9094\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1565 - acc: 0.9464 - val_loss: 0.2510 - val_acc: 0.9125\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1502 - acc: 0.9490 - val_loss: 0.2883 - val_acc: 0.9158\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1667 - acc: 0.9392 - val_loss: 0.2989 - val_acc: 0.9053\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1508 - acc: 0.9468 - val_loss: 0.3446 - val_acc: 0.9094\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1561 - acc: 0.9493 - val_loss: 0.3283 - val_acc: 0.9131\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1454 - acc: 0.9476 - val_loss: 0.3366 - val_acc: 0.9101\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1399 - acc: 0.9498 - val_loss: 0.3934 - val_acc: 0.8982\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1506 - acc: 0.9506 - val_loss: 0.3079 - val_acc: 0.8975\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1499 - acc: 0.9472 - val_loss: 0.2949 - val_acc: 0.9158\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1446 - acc: 0.9480 - val_loss: 0.4585 - val_acc: 0.9013\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1278 - acc: 0.9544 - val_loss: 0.3210 - val_acc: 0.9213\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1440 - acc: 0.9513 - val_loss: 0.3227 - val_acc: 0.9104\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1453 - acc: 0.9510 - val_loss: 0.3629 - val_acc: 0.9152\n",
      "128\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 1.2113 - acc: 0.4737 - val_loss: 1.0083 - val_acc: 0.5969\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.9496 - acc: 0.5998 - val_loss: 0.9578 - val_acc: 0.6291\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.7320 - acc: 0.6809 - val_loss: 0.8992 - val_acc: 0.5579\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.7228 - acc: 0.6858 - val_loss: 0.8388 - val_acc: 0.6586\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.5745 - acc: 0.7764 - val_loss: 0.5769 - val_acc: 0.7883\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.3838 - acc: 0.8764 - val_loss: 0.4777 - val_acc: 0.8500\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.2526 - acc: 0.9132 - val_loss: 0.4961 - val_acc: 0.8578\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.2171 - acc: 0.9257 - val_loss: 0.4479 - val_acc: 0.8738\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1953 - acc: 0.9313 - val_loss: 0.4430 - val_acc: 0.8979\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1978 - acc: 0.9329 - val_loss: 0.4236 - val_acc: 0.8972\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1735 - acc: 0.9402 - val_loss: 0.3880 - val_acc: 0.8806\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1570 - acc: 0.9444 - val_loss: 0.4655 - val_acc: 0.8924\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1640 - acc: 0.9442 - val_loss: 0.4904 - val_acc: 0.8897\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1576 - acc: 0.9444 - val_loss: 0.3029 - val_acc: 0.9101\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1559 - acc: 0.9461 - val_loss: 0.4573 - val_acc: 0.9053\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1442 - acc: 0.9483 - val_loss: 0.4402 - val_acc: 0.8965\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1346 - acc: 0.9491 - val_loss: 0.3469 - val_acc: 0.9077\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1460 - acc: 0.9436 - val_loss: 0.4724 - val_acc: 0.9046\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1391 - acc: 0.9479 - val_loss: 0.3519 - val_acc: 0.8768\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.1382 - acc: 0.9467 - val_loss: 0.3299 - val_acc: 0.8992\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1320 - acc: 0.9480 - val_loss: 0.3789 - val_acc: 0.9094\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.1562 - acc: 0.9493 - val_loss: 0.3892 - val_acc: 0.9169\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1498 - acc: 0.9470 - val_loss: 0.4978 - val_acc: 0.9101\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.1333 - acc: 0.9517 - val_loss: 0.4972 - val_acc: 0.9070\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1356 - acc: 0.9506 - val_loss: 0.5237 - val_acc: 0.8992\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1332 - acc: 0.9528 - val_loss: 0.4224 - val_acc: 0.9145\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.1488 - acc: 0.9480 - val_loss: 0.9138 - val_acc: 0.8663\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1579 - acc: 0.9446 - val_loss: 0.4809 - val_acc: 0.9026\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1423 - acc: 0.9455 - val_loss: 0.5609 - val_acc: 0.9077\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1287 - acc: 0.9498 - val_loss: 0.5919 - val_acc: 0.8999\n",
      "256\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.4210 - acc: 0.3958 - val_loss: 1.2514 - val_acc: 0.5012\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 1.0896 - acc: 0.4981 - val_loss: 1.2842 - val_acc: 0.4136\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.8790 - acc: 0.5846 - val_loss: 0.7335 - val_acc: 0.6342\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.6289 - acc: 0.7036 - val_loss: 0.7688 - val_acc: 0.6759\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.5693 - acc: 0.7806 - val_loss: 0.6819 - val_acc: 0.7906\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.3220 - acc: 0.8841 - val_loss: 0.4454 - val_acc: 0.8436\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.2217 - acc: 0.9197 - val_loss: 0.3490 - val_acc: 0.8958\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2110 - acc: 0.9278 - val_loss: 0.3097 - val_acc: 0.9026\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1816 - acc: 0.9327 - val_loss: 0.3324 - val_acc: 0.9019\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1670 - acc: 0.9397 - val_loss: 0.7032 - val_acc: 0.8711\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1818 - acc: 0.9365 - val_loss: 0.2668 - val_acc: 0.9220\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 83s 11ms/step - loss: 0.1591 - acc: 0.9393 - val_loss: 0.4378 - val_acc: 0.9040\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 84s 11ms/step - loss: 0.1574 - acc: 0.9425 - val_loss: 0.3940 - val_acc: 0.9182\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1712 - acc: 0.9431 - val_loss: 0.2501 - val_acc: 0.9155\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1497 - acc: 0.9457 - val_loss: 0.2543 - val_acc: 0.9121\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1440 - acc: 0.9450 - val_loss: 0.3149 - val_acc: 0.9104\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1489 - acc: 0.9456 - val_loss: 0.4943 - val_acc: 0.9121\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1353 - acc: 0.9476 - val_loss: 0.4071 - val_acc: 0.8921\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1457 - acc: 0.9427 - val_loss: 0.3451 - val_acc: 0.9125\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1630 - acc: 0.9448 - val_loss: 0.2518 - val_acc: 0.9165\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1449 - acc: 0.9482 - val_loss: 0.5208 - val_acc: 0.9114\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1769 - acc: 0.9381 - val_loss: 0.4005 - val_acc: 0.8816\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1630 - acc: 0.9416 - val_loss: 0.2759 - val_acc: 0.9240\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1545 - acc: 0.9419 - val_loss: 0.2708 - val_acc: 0.9186\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1481 - acc: 0.9468 - val_loss: 0.2662 - val_acc: 0.9216\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1568 - acc: 0.9456 - val_loss: 0.4181 - val_acc: 0.9155\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 85s 12ms/step - loss: 0.1524 - acc: 0.9470 - val_loss: 0.3159 - val_acc: 0.9094\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1543 - acc: 0.9480 - val_loss: 0.3217 - val_acc: 0.9162\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1488 - acc: 0.9461 - val_loss: 0.3313 - val_acc: 0.9162\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1356 - acc: 0.9478 - val_loss: 0.2741 - val_acc: 0.9237\n"
     ]
    }
   ],
   "source": [
    "lu=[8,16,64,128,256]\n",
    "for n_hidden in lu:\n",
    "  print(n_hidden)\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(n_classes, activation='sigmoid'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "  model.fit(X_train, Y_train, batch_size=batch_size, validation_data=(X_test, Y_test), epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsAH5CX9ofEb"
   },
   "source": [
    "256 lstm units give the most accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s5ltj5iAIBdX",
    "outputId": "71e082f5-6f93-4527-c3ca-c8a8296d9205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 90s 12ms/step - loss: 1.3455 - acc: 0.4166 - val_loss: 1.7455 - val_acc: 0.3994\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.0051 - acc: 0.5597 - val_loss: 0.7928 - val_acc: 0.6312\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.8160 - acc: 0.6674 - val_loss: 0.9453 - val_acc: 0.6647\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.5174 - acc: 0.8014 - val_loss: 0.8052 - val_acc: 0.5993\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.3821 - acc: 0.8679 - val_loss: 0.3723 - val_acc: 0.8744\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.2345 - acc: 0.9170 - val_loss: 0.2501 - val_acc: 0.9046\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.2180 - acc: 0.9195 - val_loss: 0.2681 - val_acc: 0.9077\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1864 - acc: 0.9295 - val_loss: 0.3798 - val_acc: 0.8694\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1717 - acc: 0.9361 - val_loss: 0.3331 - val_acc: 0.9030\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1805 - acc: 0.9338 - val_loss: 0.2118 - val_acc: 0.9162\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1535 - acc: 0.9441 - val_loss: 0.2621 - val_acc: 0.9141\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1533 - acc: 0.9441 - val_loss: 0.2330 - val_acc: 0.9230\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1585 - acc: 0.9460 - val_loss: 0.3462 - val_acc: 0.9111\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1493 - acc: 0.9463 - val_loss: 0.2465 - val_acc: 0.9325\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1712 - acc: 0.9392 - val_loss: 0.2175 - val_acc: 0.9233\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1461 - acc: 0.9460 - val_loss: 0.2209 - val_acc: 0.9267\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1720 - acc: 0.9353 - val_loss: 0.7332 - val_acc: 0.7520\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1602 - acc: 0.9370 - val_loss: 0.4940 - val_acc: 0.9016\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1417 - acc: 0.9465 - val_loss: 0.3134 - val_acc: 0.9131\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1477 - acc: 0.9474 - val_loss: 0.3436 - val_acc: 0.9321\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1370 - acc: 0.9510 - val_loss: 0.4436 - val_acc: 0.8955\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1344 - acc: 0.9478 - val_loss: 0.4335 - val_acc: 0.9087\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1325 - acc: 0.9499 - val_loss: 0.2908 - val_acc: 0.9338\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.2204 - acc: 0.9340 - val_loss: 0.3142 - val_acc: 0.9281\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1427 - acc: 0.9416 - val_loss: 0.5847 - val_acc: 0.8924\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1791 - acc: 0.9362 - val_loss: 0.3602 - val_acc: 0.9002\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1413 - acc: 0.9476 - val_loss: 0.4007 - val_acc: 0.9203\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1608 - acc: 0.9442 - val_loss: 0.4468 - val_acc: 0.9128\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1547 - acc: 0.9448 - val_loss: 0.4971 - val_acc: 0.8965\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1459 - acc: 0.9442 - val_loss: 0.6358 - val_acc: 0.8819\n",
      "0.5\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 1.3309 - acc: 0.4267 - val_loss: 1.0870 - val_acc: 0.5558\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.1964 - acc: 0.4823 - val_loss: 0.9293 - val_acc: 0.5748\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.9531 - acc: 0.5775 - val_loss: 0.8647 - val_acc: 0.6064\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.7984 - acc: 0.6283 - val_loss: 0.7438 - val_acc: 0.6651\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.5768 - acc: 0.7621 - val_loss: 0.7870 - val_acc: 0.7265\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.3428 - acc: 0.8776 - val_loss: 0.3474 - val_acc: 0.8843\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.2404 - acc: 0.9173 - val_loss: 0.5429 - val_acc: 0.8744\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1889 - acc: 0.9346 - val_loss: 0.4774 - val_acc: 0.8646\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1670 - acc: 0.9421 - val_loss: 0.2970 - val_acc: 0.9162\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1622 - acc: 0.9423 - val_loss: 0.2960 - val_acc: 0.9158\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1540 - acc: 0.9456 - val_loss: 0.3265 - val_acc: 0.9026\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1557 - acc: 0.9441 - val_loss: 0.3056 - val_acc: 0.9209\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1655 - acc: 0.9440 - val_loss: 0.4353 - val_acc: 0.9104\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1456 - acc: 0.9465 - val_loss: 0.3693 - val_acc: 0.9060\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1583 - acc: 0.9446 - val_loss: 0.3480 - val_acc: 0.9023\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1358 - acc: 0.9491 - val_loss: 0.3836 - val_acc: 0.9162\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1481 - acc: 0.9465 - val_loss: 0.3204 - val_acc: 0.9148\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1468 - acc: 0.9449 - val_loss: 0.4607 - val_acc: 0.9135\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1488 - acc: 0.9446 - val_loss: 0.3265 - val_acc: 0.8985\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1515 - acc: 0.9467 - val_loss: 0.3536 - val_acc: 0.9179\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1519 - acc: 0.9483 - val_loss: 0.3350 - val_acc: 0.9013\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1286 - acc: 0.9495 - val_loss: 0.4792 - val_acc: 0.8653\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1770 - acc: 0.9389 - val_loss: 0.3748 - val_acc: 0.9094\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1389 - acc: 0.9493 - val_loss: 0.3633 - val_acc: 0.9172\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1426 - acc: 0.9470 - val_loss: 0.3907 - val_acc: 0.8935\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1347 - acc: 0.9491 - val_loss: 0.3426 - val_acc: 0.9250\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.2115 - acc: 0.9297 - val_loss: 0.3537 - val_acc: 0.9162\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1532 - acc: 0.9489 - val_loss: 0.3276 - val_acc: 0.9253\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1510 - acc: 0.9455 - val_loss: 0.4006 - val_acc: 0.9277\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1391 - acc: 0.9464 - val_loss: 0.3787 - val_acc: 0.9206\n",
      "0.6\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 1.3754 - acc: 0.4109 - val_loss: 1.1337 - val_acc: 0.5297\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 1.1824 - acc: 0.4841 - val_loss: 0.8441 - val_acc: 0.6128\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 1.2065 - acc: 0.4460 - val_loss: 1.3235 - val_acc: 0.4961\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.7123 - acc: 0.6819 - val_loss: 0.6874 - val_acc: 0.7557\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.4187 - acc: 0.8535 - val_loss: 0.4359 - val_acc: 0.8456\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2967 - acc: 0.8970 - val_loss: 0.4077 - val_acc: 0.8812\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2195 - acc: 0.9256 - val_loss: 0.2320 - val_acc: 0.9135\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.3217 - acc: 0.8968 - val_loss: 0.3616 - val_acc: 0.8476\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2126 - acc: 0.9272 - val_loss: 0.2876 - val_acc: 0.8962\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1702 - acc: 0.9385 - val_loss: 0.4195 - val_acc: 0.8887\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1709 - acc: 0.9366 - val_loss: 0.2384 - val_acc: 0.9145\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1844 - acc: 0.9406 - val_loss: 0.3101 - val_acc: 0.9203\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1688 - acc: 0.9399 - val_loss: 0.4630 - val_acc: 0.9125\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1735 - acc: 0.9403 - val_loss: 0.2931 - val_acc: 0.9111\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1571 - acc: 0.9425 - val_loss: 0.2489 - val_acc: 0.9277\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1532 - acc: 0.9460 - val_loss: 0.3483 - val_acc: 0.9172\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1490 - acc: 0.9430 - val_loss: 0.4964 - val_acc: 0.9199\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 0.1459 - acc: 0.9441 - val_loss: 0.2384 - val_acc: 0.9298\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1469 - acc: 0.9456 - val_loss: 0.3290 - val_acc: 0.9101\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1534 - acc: 0.9426 - val_loss: 0.5651 - val_acc: 0.8758\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1569 - acc: 0.9457 - val_loss: 0.3757 - val_acc: 0.9155\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1694 - acc: 0.9440 - val_loss: 0.4882 - val_acc: 0.9104\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1710 - acc: 0.9440 - val_loss: 0.3127 - val_acc: 0.9118\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 86s 12ms/step - loss: 0.1584 - acc: 0.9434 - val_loss: 0.2193 - val_acc: 0.9274\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1603 - acc: 0.9455 - val_loss: 0.4263 - val_acc: 0.9121\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1583 - acc: 0.9464 - val_loss: 0.4939 - val_acc: 0.9155\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1733 - acc: 0.9421 - val_loss: 0.5162 - val_acc: 0.9199\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1516 - acc: 0.9479 - val_loss: 0.4223 - val_acc: 0.9182\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.1434 - acc: 0.9493 - val_loss: 0.5968 - val_acc: 0.9019\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.5045 - acc: 0.7801 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "0.7\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 1.2807 - acc: 0.4566 - val_loss: 1.2355 - val_acc: 0.4391\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.0922 - acc: 0.5224 - val_loss: 1.5276 - val_acc: 0.2779\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 1.1858 - acc: 0.4785 - val_loss: 1.3648 - val_acc: 0.4242\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.8738 - acc: 0.6004 - val_loss: 0.8422 - val_acc: 0.6502\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.7123 - acc: 0.7055 - val_loss: 0.7483 - val_acc: 0.6661\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.4595 - acc: 0.8502 - val_loss: 0.4087 - val_acc: 0.8656\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.3447 - acc: 0.8915 - val_loss: 0.4948 - val_acc: 0.8663\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2403 - acc: 0.9180 - val_loss: 0.2995 - val_acc: 0.8999\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 0.2354 - acc: 0.9218 - val_loss: 0.3702 - val_acc: 0.8677\n",
      "Epoch 10/30\n",
      "6736/7352 [==========================>...] - ETA: 6s - loss: 0.2047 - acc: 0.9279"
     ]
    }
   ],
   "source": [
    "dr=[0.4,0.5,0.6]\n",
    "for drr in dr:\n",
    "  print(drr)\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(256, input_shape=(timesteps, input_dim)))\n",
    "  model.add(Dropout(drr))\n",
    "  model.add(Dense(n_classes, activation='sigmoid'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "  model.fit(X_train, Y_train, batch_size=batch_size, validation_data=(X_test, Y_test), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XbRRmJg9LZiO",
    "outputId": "00be3792-9d94-4d99-ee0c-fb0221a68338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 1.2932 - acc: 0.4331 - val_loss: 1.4043 - val_acc: 0.4445\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.0410 - acc: 0.5491 - val_loss: 0.8182 - val_acc: 0.5765\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.8322 - acc: 0.6099 - val_loss: 0.7209 - val_acc: 0.6084\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.6272 - acc: 0.7476 - val_loss: 0.4798 - val_acc: 0.8324\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.4001 - acc: 0.8682 - val_loss: 0.5695 - val_acc: 0.8487\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.2976 - acc: 0.9083 - val_loss: 0.3906 - val_acc: 0.8816\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.3032 - acc: 0.8889 - val_loss: 0.4826 - val_acc: 0.9050\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.2577 - acc: 0.9143 - val_loss: 0.4075 - val_acc: 0.8931\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.2119 - acc: 0.9338 - val_loss: 0.3381 - val_acc: 0.8904\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.1719 - acc: 0.9388 - val_loss: 0.4871 - val_acc: 0.8897\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1774 - acc: 0.9403 - val_loss: 0.3680 - val_acc: 0.9009\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1828 - acc: 0.9422 - val_loss: 0.4745 - val_acc: 0.9087\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1687 - acc: 0.9442 - val_loss: 0.7430 - val_acc: 0.8690\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.1740 - acc: 0.9437 - val_loss: 0.6728 - val_acc: 0.9026\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.1740 - acc: 0.9378 - val_loss: 0.4353 - val_acc: 0.9063\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.1569 - acc: 0.9440 - val_loss: 0.4016 - val_acc: 0.9101\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.1767 - acc: 0.9392 - val_loss: 0.5216 - val_acc: 0.9074\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1935 - acc: 0.9410 - val_loss: 0.5565 - val_acc: 0.8996\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1545 - acc: 0.9452 - val_loss: 0.6300 - val_acc: 0.9040\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1863 - acc: 0.9384 - val_loss: 0.4814 - val_acc: 0.9070\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.1434 - acc: 0.9456 - val_loss: 0.5077 - val_acc: 0.8972\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.2335 - acc: 0.9305 - val_loss: 0.7718 - val_acc: 0.8683\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.0954 - acc: 0.5027 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "0.8\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.4002 - acc: 0.4015 - val_loss: 1.4364 - val_acc: 0.3845\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 1.0913 - acc: 0.5437 - val_loss: 1.1168 - val_acc: 0.5473\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.7785 - acc: 0.6457 - val_loss: 0.6773 - val_acc: 0.6848\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 117s 16ms/step - loss: 0.6344 - acc: 0.7410 - val_loss: 0.5562 - val_acc: 0.8015\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.4347 - acc: 0.8361 - val_loss: 0.4316 - val_acc: 0.8514\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.3575 - acc: 0.8834 - val_loss: 0.3549 - val_acc: 0.8992\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.2841 - acc: 0.9140 - val_loss: 0.4330 - val_acc: 0.8965\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.2314 - acc: 0.9259 - val_loss: 0.3330 - val_acc: 0.9057\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.1934 - acc: 0.9350 - val_loss: 0.3822 - val_acc: 0.9121\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.1874 - acc: 0.9392 - val_loss: 0.3767 - val_acc: 0.9067\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.2648 - acc: 0.9204 - val_loss: 0.3553 - val_acc: 0.9091\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.1938 - acc: 0.9372 - val_loss: 0.3160 - val_acc: 0.9172\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.1914 - acc: 0.9374 - val_loss: 0.5052 - val_acc: 0.8317\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 121s 17ms/step - loss: 0.2081 - acc: 0.9372 - val_loss: 0.6455 - val_acc: 0.8911\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.2067 - acc: 0.9377 - val_loss: 0.6347 - val_acc: 0.8972\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.4528 - acc: 0.8139 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n"
     ]
    }
   ],
   "source": [
    "dr=[0.7,0.8]\n",
    "for drr in dr:\n",
    "  print(drr)\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(256, input_shape=(timesteps, input_dim)))\n",
    "  model.add(Dropout(drr))\n",
    "  model.add(Dense(n_classes, activation='sigmoid'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "  model.fit(X_train, Y_train, batch_size=batch_size, validation_data=(X_test, Y_test), epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MY_9zPIe_aZK"
   },
   "source": [
    "0.4 dropout rate gives highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "OkYD9VYEpiRb",
    "outputId": "ebd67486-90a3-44b7-b4da-c502fe751c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128, 64)           18944     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 31,558\n",
      "Trainable params: 31,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(timesteps, input_dim),return_sequences=True))\n",
    "model.add(Dropout(0.65))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.65))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qsrMG4xpXudU",
    "outputId": "7e70f6b1-9330-432f-caf9-6fa414d5aad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 235s 32ms/step - loss: 1.2478 - acc: 0.4959 - val_loss: 1.0004 - val_acc: 0.5972\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 232s 32ms/step - loss: 0.9087 - acc: 0.6299 - val_loss: 0.7938 - val_acc: 0.6837\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 231s 31ms/step - loss: 0.7754 - acc: 0.7031 - val_loss: 0.7901 - val_acc: 0.6373\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 233s 32ms/step - loss: 0.6622 - acc: 0.7394 - val_loss: 0.6739 - val_acc: 0.6936\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 232s 32ms/step - loss: 0.5545 - acc: 0.7764 - val_loss: 0.8614 - val_acc: 0.7011\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 231s 31ms/step - loss: 0.5255 - acc: 0.7847 - val_loss: 0.4699 - val_acc: 0.7872\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 234s 32ms/step - loss: 0.4575 - acc: 0.8245 - val_loss: 0.4801 - val_acc: 0.8578\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 232s 32ms/step - loss: 0.4258 - acc: 0.8546 - val_loss: 0.4762 - val_acc: 0.8466\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 230s 31ms/step - loss: 0.3652 - acc: 0.8942 - val_loss: 0.2919 - val_acc: 0.8873\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 229s 31ms/step - loss: 0.2781 - acc: 0.9176 - val_loss: 0.3968 - val_acc: 0.8809\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 229s 31ms/step - loss: 0.2565 - acc: 0.9252 - val_loss: 0.4836 - val_acc: 0.8728\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 229s 31ms/step - loss: 0.2544 - acc: 0.9263 - val_loss: 0.5459 - val_acc: 0.8792\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 229s 31ms/step - loss: 0.2529 - acc: 0.9289 - val_loss: 0.4349 - val_acc: 0.8972\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 231s 31ms/step - loss: 0.2289 - acc: 0.9342 - val_loss: 0.3373 - val_acc: 0.9220\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 230s 31ms/step - loss: 0.2322 - acc: 0.9325 - val_loss: 0.3220 - val_acc: 0.9040\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 229s 31ms/step - loss: 0.1939 - acc: 0.9369 - val_loss: 0.3216 - val_acc: 0.9175\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 230s 31ms/step - loss: 0.1990 - acc: 0.9404 - val_loss: 0.5030 - val_acc: 0.8907\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 230s 31ms/step - loss: 0.2121 - acc: 0.9339 - val_loss: 0.5367 - val_acc: 0.9006\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 232s 32ms/step - loss: 0.1951 - acc: 0.9389 - val_loss: 0.3837 - val_acc: 0.9111\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 232s 32ms/step - loss: 0.2388 - acc: 0.9365 - val_loss: 0.4323 - val_acc: 0.9104\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 233s 32ms/step - loss: 0.2051 - acc: 0.9382 - val_loss: 0.3972 - val_acc: 0.9152\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 233s 32ms/step - loss: 0.2086 - acc: 0.9410 - val_loss: 0.4050 - val_acc: 0.9070\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 233s 32ms/step - loss: 0.1804 - acc: 0.9412 - val_loss: 0.4190 - val_acc: 0.9311\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 232s 31ms/step - loss: 0.1948 - acc: 0.9425 - val_loss: 0.4331 - val_acc: 0.9046\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 234s 32ms/step - loss: 0.3392 - acc: 0.8672 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 234s 32ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 234s 32ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 233s 32ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 234s 32ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 231s 31ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4322095d30>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, validation_data=(X_test, Y_test), epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjiZb9QTah3r"
   },
   "source": [
    "Of all our models, single layer LSTM with 256 units and dropout rate of 0.4 gives us the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ig4oYYWxbOrj",
    "outputId": "c1c057a8-81a6-4c36-fe72-21d0f49a985a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "7352/7352 [==============================] - 87s 12ms/step - loss: 1.4140 - acc: 0.3834 - val_loss: 1.4226 - val_acc: 0.4174\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.41737, saving model to weights_copy.best.hdf5\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 1.1025 - acc: 0.5101 - val_loss: 0.7636 - val_acc: 0.6512\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.41737 to 0.65117, saving model to weights_copy.best.hdf5\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.6284 - acc: 0.7348 - val_loss: 0.6704 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.65117 to 0.72514, saving model to weights_copy.best.hdf5\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.3524 - acc: 0.8739 - val_loss: 0.3381 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.72514 to 0.87377, saving model to weights_copy.best.hdf5\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.2490 - acc: 0.9082 - val_loss: 0.5114 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.87377\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.2106 - acc: 0.9257 - val_loss: 0.5300 - val_acc: 0.8456\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.87377\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1870 - acc: 0.9275 - val_loss: 0.4592 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.87377 to 0.89515, saving model to weights_copy.best.hdf5\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.2065 - acc: 0.9286 - val_loss: 0.5045 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.89515\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.1827 - acc: 0.9308 - val_loss: 0.4513 - val_acc: 0.8521\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89515\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1641 - acc: 0.9368 - val_loss: 0.3115 - val_acc: 0.9043\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.89515 to 0.90431, saving model to weights_copy.best.hdf5\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1501 - acc: 0.9419 - val_loss: 0.5116 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.90431\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1562 - acc: 0.9422 - val_loss: 0.7833 - val_acc: 0.8405\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.90431\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.1559 - acc: 0.9433 - val_loss: 0.3064 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.90431\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1442 - acc: 0.9461 - val_loss: 0.2479 - val_acc: 0.9352\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.90431 to 0.93519, saving model to weights_copy.best.hdf5\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1544 - acc: 0.9425 - val_loss: 0.2699 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.93519\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1277 - acc: 0.9482 - val_loss: 0.4210 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.93519\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.1457 - acc: 0.9440 - val_loss: 0.3279 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.93519\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1608 - acc: 0.9440 - val_loss: 0.3796 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.93519\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1679 - acc: 0.9465 - val_loss: 0.5756 - val_acc: 0.8941\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.93519\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1432 - acc: 0.9457 - val_loss: 0.3821 - val_acc: 0.9243\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.93519\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1416 - acc: 0.9495 - val_loss: 0.3148 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.93519\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1373 - acc: 0.9493 - val_loss: 0.2691 - val_acc: 0.9260\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.93519\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1294 - acc: 0.9508 - val_loss: 0.3940 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.93519\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1383 - acc: 0.9491 - val_loss: 0.5365 - val_acc: 0.9016\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.93519\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.1374 - acc: 0.9476 - val_loss: 0.3789 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.93519 to 0.93688, saving model to weights_copy.best.hdf5\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1281 - acc: 0.9502 - val_loss: 0.4654 - val_acc: 0.9080\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.93688\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1443 - acc: 0.9499 - val_loss: 0.3976 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.93688\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 77s 10ms/step - loss: 0.1891 - acc: 0.9395 - val_loss: 0.4516 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.93688\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.1387 - acc: 0.9502 - val_loss: 0.5538 - val_acc: 0.9186\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.93688\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 77s 10ms/step - loss: 0.1558 - acc: 0.9476 - val_loss: 0.5310 - val_acc: 0.9186\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.93688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66a45e6e10>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(timesteps, input_dim)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "filepath='weights_copy.best.hdf5'\n",
    "checkpoint_1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "cbl=[checkpoint_1]\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, validation_data=(X_test, Y_test), epochs=epochs,callbacks=cbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ou7u6zxXcPkz"
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights_copy.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "CFJpq2GYoTRh",
    "outputId": "b71b6570-ea32-4a28-a625-de103e4e7b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 537        0  ...                   0                 0\n",
      "SITTING                  0      401  ...                   0                 2\n",
      "STANDING                 0       54  ...                   0                 0\n",
      "WALKING                  0        1  ...                  26                 1\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 420                 0\n",
      "WALKING_UPSTAIRS         0        1  ...                   9               459\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8Hi-rMAoqP5Y",
    "outputId": "5993c708-3f82-4ebe-9dfc-1a111234c10b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DyhzYnELqX8c",
    "outputId": "1b85940d-e470-4f20-95f5-0788b0acc992"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37891137746417447, 0.9368849677638276]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDAPEYv9qcnc"
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "RYNy3Cz_ClGB",
    "outputId": "1d508d11-b741-416f-c8e1-9a655e49e287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+-----------+----------+\n",
      "|                            model                             | train_acc | test_acc |\n",
      "+--------------------------------------------------------------+-----------+----------+\n",
      "|         model 1(1 LSTM layer with tuned LSTM units)          |   0.9446  |  0.8941  |\n",
      "| model 2(1 LSTM layer with tuned LSTM units and Dropout Rate) |   0.9476  | 0.936885 |\n",
      "|       model 3(2 LSTM Layers with higher dropout rate)        |   0.9342  |  0.922   |\n",
      "+--------------------------------------------------------------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "pt=PrettyTable()\n",
    "pt.field_names=['model','train_acc','test_acc']\n",
    "pt.add_row(['model 1(1 LSTM layer with tuned LSTM units)','0.9446','0.8941'])\n",
    "pt.add_row(['model 2(1 LSTM layer with tuned LSTM units and Dropout Rate)','0.9476','0.936885'])\n",
    "pt.add_row(['model 3(2 LSTM Layers with higher dropout rate)','0.9342','0.922'])\n",
    "\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHdvmagOqgWI"
   },
   "source": [
    "With single LSTM layer with 256 LSTM units and dropout rate of 0.4, we are able to get test accuracy of 93.6885%. So clearly, our model has improved from its original 89.41% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkRxKql7qY4H"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HAR_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
